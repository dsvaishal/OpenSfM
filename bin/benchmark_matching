#!/usr/bin/env python

import argparse
import datetime
import logging
import os.path
import shutil
import yaml

from itertools import combinations
from timeit import default_timer as timer

from opensfm import bow
from opensfm import commands
from opensfm import dataset
from opensfm import features
from opensfm import io
from opensfm import log
from opensfm import matching


logger = logging.getLogger(__name__)
log.setup()


def create_bench_dir(dataset):
    dataset_dir = os.path.basename(os.path.normpath(dataset))

    base_dir = os.path.dirname(os.path.dirname(dataset)) if \
        os.path.isdir(dataset) else \
        os.path.dirname(dataset)

    bench_dir = os.path.join(
        base_dir,
        dataset_dir + \
        "_bench_feat_match_" + \
        datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))

    io.mkdir_p(bench_dir)

    return bench_dir


def create_brute_force_dataset(orig_dir, bench_dir):
    work_dir = os.path.join(bench_dir, "bruteforce")
    shutil.copytree(orig_dir, work_dir)

    config = load_config(work_dir)
    config["matcher_type"] = "BRUTEFORCE"
    save_config(config, work_dir)

    parser = argparse.ArgumentParser()
    parser.add_argument('dataset')
    args = parser.parse_args([work_dir])

    return dataset.DataSet(args.dataset)


def create_flann_dataset(orig_dir, bench_dir):
    work_dir = os.path.join(bench_dir, "flann")
    shutil.copytree(orig_dir, work_dir)

    config = load_config(work_dir)
    config["matcher_type"] = "FLANN"
    save_config(config, work_dir)

    parser = argparse.ArgumentParser()
    parser.add_argument('dataset')
    args = parser.parse_args([work_dir])

    data = dataset.DataSet(args.dataset)

    for image in data.images():
        p, f, c = data.load_features(image)
        index = features.build_flann_index(f, data.config)
        data.save_feature_index(image, index)

    return data


def create_words_dataset(orig_dir, bench_dir):
    work_dir = os.path.join(bench_dir, "words")
    shutil.copytree(orig_dir, work_dir)

    config = load_config(work_dir)
    config["matcher_type"] = "WORDS"
    save_config(config, work_dir)

    parser = argparse.ArgumentParser()
    parser.add_argument('dataset')
    args = parser.parse_args([work_dir])

    data = dataset.DataSet(args.dataset)

    for image in data.images():
        bows = bow.load_bows(data.config)
        n_closest = data.config['bow_words_to_match']
        p, f, c = data.load_features(image)
        closest_words = bows.map_to_words(
            f, n_closest, data.config['bow_matcher_type'])
        data.save_words(image, closest_words)

    return data


def load_config(work_dir):
    config = {}
    filepath = os.path.join(work_dir, "config.yaml")
    if os.path.isfile(filepath):
        with open(filepath) as fin:
            new_config = yaml.safe_load(fin)
        if new_config:
            for k, v in new_config.items():
                config[k] = v

    return config


def save_config(config, work_dir):
    with open(os.path.join(work_dir, 'config.yaml'), 'w') as fout:
        yaml.dump(config, fout, default_flow_style=False)


def match(data):
    images = data.images()
    images.sort()

    pairs = combinations(images, 2)
    comb = {im: [] for im in images}
    for im1, im2 in pairs:
        comb[im1].append(im2)

    matcher_type = data.config['matcher_type']


    for im1 in comb:
        im1_matches = {}

        for im2 in comb[im1]:
            t = timer()

            # symmetric matching
            t = timer()
            p1, f1, c1 = data.load_features(im1)
            p2, f2, c2 = data.load_features(im2)

            if matcher_type == 'WORDS':
                w1 = data.load_words(im1)
                w2 = data.load_words(im2)
                matches = matching.match_words_symmetric(
                    f1, w1, f2, w2,data.config)
            elif matcher_type == 'FLANN':
                i1 = data.load_feature_index(im1, f1)
                i2 = data.load_feature_index(im2, f2)
                matches = matching.match_flann_symmetric(
                    f1, i1, f2, i2,data.config)
            elif matcher_type == 'BRUTEFORCE':
                matches = matching.match_brute_force_symmetric(
                    f1, f2,data.config)
            else:
                raise ValueError("Invalid matcher_type: {}".format(
                    matcher_type))

            im1_matches[im2] = matches

            logger.debug('{} - {} has {} initial matches'.format(
                im1, im2, len(matches)))

            logger.info('Matching time ({0}, {1}-{2}, {3}-{4}): {5}s'.format(
                matcher_type, im1, im2, len(f1), len(f2), timer() - t))

        data.save_matches(im1, im1_matches)


def load_matches(data):
    matches = {}
    for im1 in data.images():
        m = data.load_matches(im1)
        for im2 in m:
            matches[tuple(sorted([im1, im2]))] = m[im2]

    return matches


def benchmark_matches(matches, truth):
    res = {
        "true_pos": [],
        "false_pos": [],
        "false_neg": [],
    }

    for pair in truth:
        if pair not in matches:
            res["false_neg"] = truth[pair]
            continue

        m = matches[pair]
        for match in truth[pair]:
            if match in m:
                res["true_pos"].append(match)
            else:
                res["false_neg"].append(match)

    for pair in matches:
        if pair not in truth:
            res["false_pos"] = matches[pair]
            continue

        t = truth[pair]
        for match in matches[pair]:
            if match not in t:
                res["false_pos"].append(match)

    return res


def report(res, method_name):
    true_pos = len(res["true_pos"])
    false_pos = len(res["false_pos"])
    false_neg = len(res["false_neg"])

    logger.info("Report for {}".format(method_name))
    logger.info("Precision: {}".format(float(true_pos) / (true_pos + false_pos)))
    logger.info("Recall: {}".format(float(true_pos) / (true_pos + false_neg)))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Benchmark feature matching algorithms')
    parser.add_argument('dataset',
                        help='path to the dataset to be processed')

    args = parser.parse_args()

    commands.extract_metadata.Command().run(args)
    commands.detect_features.Command().run(args)

    bench_dir = create_bench_dir(args.dataset)

    b_data = create_brute_force_dataset(args.dataset, bench_dir)
    f_data = create_flann_dataset(args.dataset, bench_dir)
    w_data = create_words_dataset(args.dataset, bench_dir)

    match(b_data)
    match(f_data)
    match(w_data)

    b_matches = load_matches(b_data)
    f_matches = load_matches(f_data)
    w_matches = load_matches(w_data)

    f_res = benchmark_matches(f_matches, b_matches)
    w_res = benchmark_matches(w_matches, b_matches)

    report(f_res, f_data.config["matcher_type"])
    report(w_res, w_data.config["matcher_type"])
